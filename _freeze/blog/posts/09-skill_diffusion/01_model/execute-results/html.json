{
  "hash": "7b3aeed108f0c6eb0d7a552b2772ebe0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Structurally Conditioned Diffusion: A Generalized Framework for Asymmetric Trajectory Channeling in Labor Markets\"\nsubtitle: \"Integrating Piecewise Diffusion Theory with Empirical Analysis of U.S. Skills Data (2015-2023)\"\nauthor: \n  - name: \"Roberto Cantillan & Mauricio Bucca\"\n    affiliations: \n        - name: \"Department of Sociology, PUC\"\n          address: \"Santiago, Chile\"\n\ndate: today\nbibliography: paper_skills_diffusion.bib\nbibliographystyle: apa\ncategories:\n  - R\n  - Diffusion Theory\n  - Polarization\n  - Stratification \n  - Inequality\n  - Labor Markets\n \nimage: \"featured.jpg\"\ntitle-block-banner: featured.jpg\ntitle-block-banner-color: \"rgba(0, 0, 0, 0.8)\"\ntitle-block-style: default\n---\n\n\n\n\n\n\n# Introduction: From Employer Decisions to Labor Market Structure\n\nThe architecture of inequality in the U.S. labor market is not a static blueprint but an actively reproduced, dynamic process. Its foundations lie in the everyday decisions of **employers within organizations**, who determine which skill requirements to establish for the occupations they manage. Foundational studies have demonstrated that the skill landscape itself is starkly polarized into two distinct domains—a socio-cognitive cluster associated with high wages and a sensory-physical one with low wages [@alabdulkareem_unpacking_2018]—and that this space has a nested, hierarchical architecture [@hosseinioun_skill_2025]. This structural view aligns with recent findings in intergenerational mobility research, which conceptualize occupations not as monolithic categories but as complex bundles of gradational characteristics, where it is often the underlying traits, rather than the job title itself, that are transmitted across generations [@york_gradationalism_2025].\n\nFaced with uncertainty about which skill requirements will maximize productivity or prestige, employers often look to the practices of **other organizations** for guidance. They engage in a process of social learning and imitation, observing how similar organizations define skill requirements for comparable occupations. However, we argue this imitation is not random. It is governed by a powerful, yet poorly understood, mechanism of asymmetric filtering based on the **cultural theorization** of the skill itself. The nature of a skill—how it is socially interpreted and valued—fundamentally alters the pathways it can travel across the occupational status hierarchy as organizations adopt and adapt these requirements.\n\nThe cumulative result of thousands of these guided, micro-level imitation decisions is a macro-level process we term **Asymmetric Trajectory Channeling**. This process actively sorts skills into divergent mobility paths—an upward \"escalator\" for cognitive skills and a \"containment field\" for physical ones—thus deepening labor market stratification from the demand side.\n\nThis study moves beyond describing the consequences of polarization for workers (the supply side) to model the causal mechanisms on the demand side that generate it. By focusing on the rules that guide employer imitation **between organizations**, we explain how structural inequality is not merely a state, but an emergent process reproduced from the ground up through organizational decision-making.\n\n# Theoretical Foundations: From Mimicry to Meaning\n\nEarly studies of diffusion often focused on the spread of a single innovation through a homogenous population. More recent work, however, recognizes that diffusion is fundamentally structured by networks and heterogeneity. Practices do not diffuse randomly or uniformly; they follow patterned trajectories shaped by organizational, cultural, and institutional constraints [@strang_diffusion_1998].\n\n## The Micro-Foundations of Imitation\n\nWe extend this insight by arguing that **employers imitate skill requirements according to fundamentally different logics depending on the type of skill in question**. To understand why, we must first examine the micro-foundations of organizational imitation. The literature suggests three key mechanisms that drive one organization to adopt the practices of another:\n\n1.  **Uncertainty and Bounded Rationality:** Under conditions of uncertainty about the relationship between means and ends, organizations often imitate others as a decision-making shortcut. Rather than calculating an optimal solution from scratch, imitation offers a viable solution with reduced search costs [@cyert_behavioral_2006; @dimaggio_iron_1983].\n\n2.  **Prestige and Status-Seeking:** Imitation is not just a response to uncertainty, but also a strategy to gain legitimacy and status. Organizations do not imitate just anyone; they emulate those they perceive to be more successful or prestigious [@strang_search_2001; @bail_prestige_2019]. This process of \"adaptive emulation\" [@strang_search_2001], driven by \"success stories,\" creates an inherent directional bias in diffusion, where practices flow from high-status to low-status actors.\n\n3.  **Proximity and Network Structure:** Influence is not global but is channeled through social and structural networks. The likelihood that one organization imitates another is strongly conditioned by proximity, whether geographic, social, or cultural [@hedstrom_contagious_1994; @strang_spatial_1993]. Actors are more influenced by their peers, direct competitors, or those with whom they maintain dense relationships.\n\n## A Dual-Process Theory of Skill Diffusion: The Role of Theorization\n\nOur key theoretical innovation is that the **content** of a skill—how it is culturally **theorized** [@strang_institutional_1993]—determines which of these micro-foundations becomes dominant. We argue that organizations filter and evaluate potential practices through two qualitatively different diffusion logics:\n\n* **Cognitive Skills as Portable Assets:** Cognitive skills (analytical, interpersonal, managerial) are theorized as **nested capabilities**: they are abstract, broadly applicable, and perceived as portable assets associated with growth, learning, and adaptability. Under uncertainty, employers look toward **prestigious exemplars** [@strang_learning_2010] that signal success and modernity. As a result, the diffusion of cognitive skills is driven primarily by **aspirational emulation**. They tend to diffuse **upward** through the occupational status hierarchy as organizations seek to imitate their high-status peers.\n\n* **Physical Skills as Context-Dependent Competencies:** In contrast, physical skills (manual, motor) are theorized as **context-dependent competencies**: they are tied to specific material settings, bodily execution, and legacy institutional constraints. They are less likely to be read as generalizable templates for upward mobility. Instead, their diffusion is based on functional rather than aspirational considerations. Therefore, the diffusion of physical skills is governed mainly by **proximity and functional need**, showing less directional bias or even **containment effects** within their current status segments.\n\nThis bifurcation in diffusion logics is what we call **Asymmetric Trajectory Channeling**. While our model, for analytical clarity, uses a binary distinction, we treat skills as **multidimensional bundles** of traits, values, and dispositions [@york_gradationalism_2025]. Employers imitate not just individual skills, but **pieces of occupational identity**.\n\n## Piecewise Dual Process Diffusion Model\n\nTo formalize these dynamics while avoiding multicollinearity issues, we model the **probability** ($P_{i \\to j}^{(c)}$) that an employer, managing a target occupation $j$, adopts a skill of class $c$ by imitating a requirement from a source occupation $i$. This decision is embedded in the macro-level structure of the occupational space.\n\n### Mathematical Formulation\n\nWe define two **non-overlapping** directional status variables that completely separate upward and downward mobility:\n\n$$\n\\Delta^+_{ij} = \\max(0, s_j - s_i) \\quad \\text{(upward status shift)}\n$$\n\n$$\n\\Delta^-_{ij} = \\max(0, s_i - s_j) \\quad \\text{(downward status shift)}\n$$\n\nThe structural form of our **Piecewise Dual Process Model** expresses the diffusion likelihood as:\n\n$$\n\\text{logit}\\, P_{i \\to j}^{(c)} = \\theta_{0c} - \\lambda_c d_{ij} - \\beta_c^+ \\Delta^+_{ij} - \\beta_c^- \\Delta^-_{ij} - \\omega_c w_{ij}\n$$\n\nWhere:\n- $\\theta_{0c}$: baseline propensity to adopt skill class $c$\n- $\\lambda_c$: structural distance penalty parameter\n- $\\beta_c^+$: penalty (or bonus) per unit **upward** status move\n- $\\beta_c^-$: penalty (or bonus) per unit **downward** status move\n- $\\omega_c$: coefficient for wage gap control $w_{ij}$\n\n### Advantages of the Piecewise Specification\n\n1.  **Eliminates Multicollinearity**: Unlike models using both $(s_j - s_i)$ and $|s_j - s_i|$, our piecewise variables are mathematically orthogonal since $\\Delta^+_{ij} \\cdot \\Delta^-_{ij} = 0$ always.\n2.  **Direct Asymmetry Testing**: The difference $(\\beta_c^+ - \\beta_c^-)$ directly measures directional asymmetry without requiring complex transformations.\n3.  **Clear Interpretation**: Each parameter has an unambiguous meaning tied to specific types of status moves.\n\n### Curved Piecewise Extension\n\nFor situations where we expect **nonlinear sensitivity** to status differences, the model can be extended to include curvature terms. We estimate a **generalized linear model** with a **Bernoulli likelihood** and **logit link**. The outcome $y_{ij}^{(c)} \\in \\{0, 1\\}$ indicates whether occupation $j$ adopts a skill of class $c$ from occupation $i$:\n\n$$\ny_{ij}^{(c)} \\sim \\text{Bernoulli}(p_{ij}^{(c)}), \\quad \\text{logit}(p_{ij}^{(c)}) = \\eta_{ij}^{(c)}\n$$\n\nThe predictor is a **second-order polynomial expansion** of upward and downward status gaps:\n\n$$\n\\eta_{ij}^{(c)} =\n\\theta_{0c}\n- \\lambda_c d_{ij}\n- \\beta_{1c}^+ \\Delta^+_{ij}\n- \\beta_{2c}^+ (\\Delta^+_{ij})^2\n- \\beta_{1c}^- \\Delta^-_{ij}\n- \\beta_{2c}^- (\\Delta^-_{ij})^2\n$$\n\n### Prior Choices: Rationale\n\n| Parameter | Description | Prior | Justification |\n|---|---|---|---|\n| $\\theta_{0c}$ (intercept) | Baseline adoption tendency | `normal(0, 5)` | Nearly uninformative; wide enough for coverage |\n| $\\lambda_c$, $\\beta_{1c}^{\\pm}$ | Main distance and status slope terms | `normal(0, 1)` | Weakly informative; encourages shrinkage |\n| $\\beta_{2c}^{\\pm}$ | Curvature terms (quadratic effects) | `double_exponential(0, 0.5)`| **Lasso-type prior** to enable variable selection |\n\nThe **double-exponential (or Laplace) prior** places more mass near 0 than the normal, encouraging sparsity in curvature terms (i.e., it can shrink them toward zero). This is helpful if you're unsure whether curvature is really needed.\n\n\n![](plot_intro.png)\n\n\n# Empirical Analysis\n\n## Data and Setup\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(kableExtra)\n  library(data.table)\n  library(ggplot2)\n  library(broom)\n  library(dplyr)\n  library(scales)\n  library(lmtest)\n  library(knitr)\n  library(patchwork)\n  library(tidyr)\n  library(ggrepel)\n  library(viridis)\n  library(RColorBrewer)\n  library(ggtext)\n  library(brms)\n  library(fixest)  # For fixed effects models\n})\n\n# Custom theme for a professional, academic look\ntheme_paper <- function(base_size = 15, base_family = \"sans\") { \n  theme_minimal(base_size = base_size, base_family = base_family) +\n    theme(\n      text = element_text(color = \"grey20\"),\n      plot.title = element_text(size = rel(1.6), face = \"bold\", hjust = 0.5, margin = margin(b = 15), family = base_family), \n      plot.subtitle = element_markdown(size = rel(1.1), color = \"grey40\", hjust = 0.5, margin = margin(b = 15), lineheight = 1.2), \n      plot.caption = element_text(size = rel(0.9), color = \"grey50\", hjust = 1, margin = margin(t = 12)), \n      axis.title = element_text(size = rel(1.25), face = \"bold\", color = \"grey30\"), \n      axis.text = element_text(size = rel(1.1), color = \"grey40\"), \n      axis.line = element_line(color = \"grey80\", linewidth = 0.6),\n      panel.grid.major = element_line(color = \"grey93\", linewidth = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.title = element_text(size = rel(1.15), face = \"bold\"), \n      legend.text = element_text(size = rel(1.05)), \n      legend.background = element_rect(fill = \"white\", color = \"grey90\"),\n      legend.key = element_rect(fill = \"white\", color = NA),\n      strip.text = element_text(size = rel(1.25), face = \"bold\", color = \"grey20\"), \n      strip.background = element_rect(fill = \"grey97\", color = \"grey80\")\n    )\n}\ntheme_set(theme_paper())\n\n# Define a consistent color palette\ncolor_palette <- c(\"Socio-cognitive\" = \"#0072B2\", \"Socio-technical\" = \"#D55E00\")\n\noutput_dir <- paste0(\"Stratified_Diffusion_Analysis_\", Sys.Date())\nif (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)\n\ncomma <- function(x) format(x, big.mark = \",\", scientific = FALSE)\n```\n:::\n\n\n\n\n## Data Preparation \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Robust data loading function\nload_and_prepare_data <- function() {\n  data_path <- \"/home/rober/Descargas/all_events_final_enriched.RData\"\n \n  if (file.exists(data_path)) {\n    load(data_path)\n    if (exists(\"all_events_final_enriched\")) {\n      data_for_models <- all_events_final_enriched\n      message(\"Successfully loaded 'all_events_final_enriched'.\")\n    } else {\n      message(\"Warning: 'all_events_final_enriched' not found. Searching for another suitable data object.\")\n      all_objs <- ls()\n      suitable_objects <- all_objs[sapply(all_objs, function(x) {\n        obj <- get(x)\n        (is.data.frame(obj) || is.data.table(obj)) && nrow(obj) > 1000\n      })]\n      if (length(suitable_objects) > 0) {\n        data_for_models <- get(suitable_objects[1])\n        message(paste(\"Using fallback data object:\", suitable_objects[1]))\n      } else {\n        stop(\"No suitable data object found.\")\n      }\n    }\n  } else {\n    stop(paste(\"Data file not found at path:\", data_path))\n  }\n \n  return(data_for_models)\n}\n\n# Load data\ndata_for_models <- load_and_prepare_data()\n```\n:::\n\n\n\n\n## Variable Construction\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Robust variable mapping - UPDATED TO USE distance_euclidean_rca consistently\navailable_cols <- names(data_for_models)\n\nrequired_mapping <- list(\n  diffusion = c(\"diffusion\", \"adopt\", \"adoption\", \"outcome\"),\n  structural_distance = c(\"distance_euclidean_rca\", \"structural_distance\", \"dist\", \"distance\", \"d_ij\"),\n  status_diff = c(\"education_diff_abs\", \"status_gap_signed\", \"education_diff\", \"status_diff\", \"edu_diff\", \"status_gap\"),\n  wage_diff = c(\"wage_diff_abs\", \"wage_gap\", \"wage_diff\", \"wage_distance\"),\n  skill_group = c(\"skill_group_for_model\", \"skill_group\", \"cluster\", \"group\", \"LeidenCluster_2015\", \"skill_cluster_type\"),\n  source_education = c(\"source_education\", \"edu_i\", \"source_edu\", \"soc_source\", \"education_source\"),\n  target_education = c(\"target_education\", \"edu_j\", \"target_edu\", \"soc_target\", \"education_target\"),\n  source_wage = c(\"source_wage\", \"wage_i\", \"wage_source\"),\n  target_wage = c(\"target_wage\", \"wage_j\", \"wage_target\")\n)\n\ncolumn_matches <- sapply(required_mapping, function(p_names) {\n  found <- intersect(p_names, available_cols)\n  if (length(found) > 0) found[1] else NA_character_\n})\n\n# Check for essential columns\nessential_cols <- c(\"diffusion\", \"structural_distance\", \"status_diff\", \"skill_group\")\nif (any(is.na(column_matches[essential_cols]))) {\n  missing_essentials <- names(column_matches[essential_cols][is.na(column_matches[essential_cols])])\n  stop(paste(\"Missing essential columns for model:\", paste(missing_essentials, collapse=\", \")))\n}\n\nif (!is.data.table(data_for_models)) {\n  data_for_models <- as.data.table(data_for_models)\n}\n\ngeneralized_data <- copy(data_for_models)\n\n# Create variables for the model - USING distance_euclidean_rca consistently\ngeneralized_data[, `:=`(\n  diffusion = as.numeric(get(column_matches[[\"diffusion\"]])),\n  d_ij = as.numeric(get(column_matches[[\"structural_distance\"]])),  # This will be distance_euclidean_rca\n  status_gap_signed = as.numeric(get(column_matches[[\"status_diff\"]])),\n  skill_group_for_model = as.character(get(column_matches[[\"skill_group\"]]))\n)]\n\n# Create piecewise variables\ngeneralized_data[, `:=`(\n  delta_up = pmax(0, status_gap_signed),      # Δ⁺: upward movements\n  delta_down = pmax(0, -status_gap_signed),  # Δ⁻: downward movements\n  status_gap_magnitude = abs(status_gap_signed)  # Keep for compatibility\n)]\n\n# Handle wage_gap column\nif (\"wage_diff_abs\" %in% names(generalized_data)) {\n  generalized_data[, wage_gap := as.numeric(wage_diff_abs)]\n  message(\"Using 'wage_diff_abs' for wage_gap.\")\n} else if (!is.na(column_matches[[\"wage_diff\"]])) {\n  generalized_data[, wage_gap := as.numeric(get(column_matches[[\"wage_diff\"]]))]\n} else {\n  message(\"Warning: 'wage_diff' column not found. Creating a dummy 'wage_gap' column.\")\n  generalized_data[, wage_gap := 0]\n}\n\n# Add source/target variables\nif (all(c(\"source_Edu_Score_Weighted\", \"target_Edu_Score_Weighted\") %in% names(generalized_data))) {\n  generalized_data[, source_edu := as.numeric(source_Edu_Score_Weighted)]\n  generalized_data[, target_edu := as.numeric(target_Edu_Score_Weighted)]\n  message(\"Using 'source_Edu_Score_Weighted' and 'target_Edu_Score_Weighted' for education variables.\")\n}\n\nif (all(c(\"source_Median_Wage_2015\", \"target_Median_Wage_2023\") %in% names(generalized_data))) {\n  generalized_data[, source_wage := as.numeric(source_Median_Wage_2015)]\n  generalized_data[, target_wage := as.numeric(target_Median_Wage_2023)]\n  message(\"Using 'source_Median_Wage_2015' and 'target_Median_Wage_2023' for wage variables.\")\n}\n\n# Assign content type based on skill group\nif (\"LeidenCluster_2015\" %in% names(generalized_data)) {\n    message(\"Applying specific corrections to 'LeidenCluster_2015' as per original script.\")\n    generalized_data[, LeidenCluster_2015 := as.character(LeidenCluster_2015)]\n    generalized_data[LeidenCluster_2015 == \"NoCluster_2015\", LeidenCluster_2015 := \"LeidenC_1\"]\n    generalized_data <- generalized_data[LeidenCluster_2015 %in% c(\"LeidenC_1\", \"LeidenC_2\")]\n    generalized_data[, content_type := fifelse(LeidenCluster_2015 == \"LeidenC_1\", \"Socio-cognitive\", \"Socio-technical\")]\n} else {\n    message(\"Warning: 'LeidenCluster_2015' not found. Using generic classification.\")\n    unique_skill_groups <- unique(generalized_data$skill_group_for_model)\n    set.seed(42)\n    groups_sorted <- sort(unique_skill_groups)\n    enhancing_groups <- head(groups_sorted, length(groups_sorted) / 2)\n    generalized_data[, content_type := fifelse(skill_group_for_model %in% enhancing_groups, \n                                                \"Socio-cognitive\", \"Socio-technical\")]\n}\n\n# Final data cleaning\ngeneralized_data <- generalized_data[!is.na(content_type)]\ngeneralized_data <- na.omit(generalized_data, \n                                cols = c(\"diffusion\", \"d_ij\", \"delta_up\", \"delta_down\", \"wage_gap\"))\n\n# Verify piecewise variables\nmessage(sprintf(\"Created piecewise variables: delta_up (mean=%.3f), delta_down (mean=%.3f)\", \n                mean(generalized_data$delta_up), mean(generalized_data$delta_down)))\n\n# Fragment data\ncognitive_data <- generalized_data[content_type == \"Socio-cognitive\"]\nphysical_data <- generalized_data[content_type == \"Socio-technical\"]\n\n# Report which distance variable is being used\ndistance_var_used <- column_matches[[\"structural_distance\"]]\nmessage(sprintf(\"Using '%s' as structural distance variable across all analyses.\", distance_var_used))\n```\n:::\n\n\n\n\n## Descriptive Analysis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndesc_stats <- generalized_data[, .(\n  N = comma(.N),\n  Diffusion_Rate = sprintf(\"%.1f%%\", mean(diffusion, na.rm = TRUE) * 100),\n  Mean_Distance = sprintf(\"%.3f\", mean(d_ij, na.rm = TRUE)),\n  Mean_Upward_Shift = sprintf(\"%.3f\", mean(delta_up, na.rm = TRUE)),\n  Mean_Downward_Shift = sprintf(\"%.3f\", mean(delta_down, na.rm = TRUE)),\n  Upward_Diffusion_Rate = sprintf(\"%.1f%%\", mean(diffusion[delta_up > 0], na.rm = TRUE) * 100),\n  Downward_Diffusion_Rate = sprintf(\"%.1f%%\", mean(diffusion[delta_down > 0], na.rm = TRUE) * 100)\n), by = content_type]\n\nkable(desc_stats, \n      caption = \"**Table 1: Descriptive Statistics by Skill Content Type**\",\n      col.names = c(\"Content Type\", \"N\", \"Diffusion Rate\", \"Mean Distance\", \n                    \"Mean Δ⁺\", \"Mean Δ⁻\", \"Upward Diffusion\", \"Downward Diffusion\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>**Table 1: Descriptive Statistics by Skill Content Type**</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Content Type </th>\n   <th style=\"text-align:left;\"> N </th>\n   <th style=\"text-align:left;\"> Diffusion Rate </th>\n   <th style=\"text-align:left;\"> Mean Distance </th>\n   <th style=\"text-align:left;\"> Mean Δ⁺ </th>\n   <th style=\"text-align:left;\"> Mean Δ⁻ </th>\n   <th style=\"text-align:left;\"> Upward Diffusion </th>\n   <th style=\"text-align:left;\"> Downward Diffusion </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Socio-cognitive </td>\n   <td style=\"text-align:left;\"> 353,988 </td>\n   <td style=\"text-align:left;\"> 26.8% </td>\n   <td style=\"text-align:left;\"> 0.679 </td>\n   <td style=\"text-align:left;\"> 1.161 </td>\n   <td style=\"text-align:left;\"> 0.773 </td>\n   <td style=\"text-align:left;\"> 37.5% </td>\n   <td style=\"text-align:left;\"> 13.6% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Socio-technical </td>\n   <td style=\"text-align:left;\"> 318,951 </td>\n   <td style=\"text-align:left;\"> 27.0% </td>\n   <td style=\"text-align:left;\"> 0.675 </td>\n   <td style=\"text-align:left;\"> 1.512 </td>\n   <td style=\"text-align:left;\"> 0.294 </td>\n   <td style=\"text-align:left;\"> 27.0% </td>\n   <td style=\"text-align:left;\"> 27.1% </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n**Table 1 Interpretation:** This table summarizes the key variables for the two skill types. We observe that socio-cognitive skills have a slightly higher overall diffusion rate. Notably, for both types, the diffusion rate is higher for upward status shifts (Δ⁺ > 0) than for downward shifts (Δ⁻ > 0), providing initial evidence of an aspirational bias in skill adoption.\n\n## Descriptive Visualizations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_descriptive_coherent_style <- function(data) {\n  \n  # Trabajar con data.table\n  if (!is.data.table(data)) {\n    dt <- as.data.table(data)\n  } else {\n    dt <- copy(data)\n  }\n  \n  # Verificar columnas\n  required_cols <- c(\"status_gap_signed\", \"content_type\", \"diffusion\")\n  missing_cols <- required_cols[!required_cols %in% names(dt)]\n  \n  if (length(missing_cols) > 0) {\n    stop(\"Missing required columns: \", paste(missing_cols, collapse = \", \"))\n  }\n  \n  # Crear movement_type\n  dt[, movement_type := fcase(\n    status_gap_signed < -0.5, \"Downward\",\n    abs(status_gap_signed) <= 0.5, \"Lateral\",\n    status_gap_signed > 0.5, \"Upward\",\n    default = NA_character_\n  )]\n  \n  # Filtrar datos válidos\n  dt_clean <- dt[!is.na(movement_type) & !is.na(content_type)]\n  \n  # Calcular estadísticas\n  summary_stats <- dt_clean[, .(\n    diffusion_rate = mean(diffusion, na.rm = TRUE),\n    n = .N\n  ), by = .(movement_type, content_type)]\n  \n  # Calcular intervalos de confianza\n  summary_stats[, `:=`(\n    se = sqrt(diffusion_rate * (1 - diffusion_rate) / n),\n    ci_lower = pmax(0, diffusion_rate - 1.96 * sqrt(diffusion_rate * (1 - diffusion_rate) / n)),\n    ci_upper = pmin(1, diffusion_rate + 1.96 * sqrt(diffusion_rate * (1 - diffusion_rate) / n))\n  )]\n  \n  # Convertir a data.frame y ordenar\n  summary_df <- as.data.frame(summary_stats)\n  summary_df$movement_type <- factor(summary_df$movement_type, \n    levels = c(\"Downward\", \"Lateral\", \"Upward\"))\n  \n  # Mismos colores que el plot de líneas\n  coherent_colors <- c(\n    \"Socio-cognitive\" = \"#1f77b4\",  # Azul coherente\n    \"Socio-technical\" = \"#ff7f0e\"   # Naranja coherente\n  )\n  \n  # Crear el plot con el mismo estilo\n  p <- ggplot(summary_df, aes(x = movement_type, y = diffusion_rate, fill = content_type)) +\n    \n    # Área sombreada sutil para destacar el escalator effect\n    annotate(\"rect\", xmin = 2.5, xmax = 3.5, ymin = -Inf, ymax = Inf, \n             fill = \"#E8F4FD\", alpha = 0.3) +  # Azul muy claro\n    \n    # Barras principales\n    geom_col(position = position_dodge(0.8), \n             width = 0.7,\n             color = \"white\", \n             linewidth = 0.8,\n             alpha = 0.85) +\n    \n    # Barras de error elegantes\n    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                  position = position_dodge(0.8), \n                  width = 0.25, \n                  linewidth = 1,\n                  color = \"grey40\") +\n    \n    # Etiquetas de porcentaje\n    geom_text(aes(label = paste0(round(diffusion_rate * 100, 1), \"%\")), \n              position = position_dodge(0.8), \n              vjust = -0.6, \n              size = 5, \n              fontface = \"bold\",\n              color = \"grey20\") +\n    \n    # Flecha sutil llamando atención al escalator effect\n    annotate(\"segment\", \n             x = 2.7, xend = 3.3, \n             y = 0.42, yend = 0.42, \n             arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n             color = \"#1f77b4\", \n             linewidth = 2,\n             alpha = 0.8) +\n    \n    # Texto explicativo sutil\n    annotate(\"text\", \n             x = 3, y = 0.45, \n             label = \"ESCALATOR\\nEFFECT\", \n             color = \"#1f77b4\", \n             fontface = \"bold\", \n             size = 4, \n             hjust = 0.5,\n             lineheight = 0.9) +\n    \n    # Escalas coherentes con el estilo\n    scale_fill_manual(values = coherent_colors, name = \"Skill Type:\") +\n    scale_y_continuous(\n      limits = c(0, 0.5),\n      breaks = seq(0, 0.5, 0.1),\n      expand = expansion(mult = c(0, 0.05)),\n      labels = function(x) paste0(x*100, \"%\")\n    ) +\n    scale_x_discrete(\n      labels = c(\"Downward\\n(High → Low)\", \"Lateral\\n(Similar Status)\", \"Upward\\n(Low → High)\")\n    ) +\n    \n    # Etiquetas coherentes\n    labs(\n      title = \"Raw Data Reveals Asymmetric Trajectory Channeling\",\n      subtitle = \"Descriptive evidence for differential skill mobility patterns by content type\",\n      x = \"Status Movement Direction\",\n      y = \"Observed Skill Adoption Rate\"\n    ) +\n    \n    # MISMO TEMA que el plot de líneas\n    theme_minimal(base_size = 15) +\n    theme(\n      # Mismo fondo gris\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      \n      # Grid sutil igual al original\n      panel.grid.major = element_line(color = \"white\", linewidth = 0.6),\n      panel.grid.minor = element_line(color = \"white\", linewidth = 0.3),\n      \n      # Sin bordes para mantener coherencia\n      panel.border = element_blank(),\n      axis.line = element_line(color = \"grey40\", linewidth = 0.5),\n      \n      # Títulos coherentes\n      plot.title = element_text(\n        size = 18, \n        face = \"bold\", \n        hjust = 0.5, \n        color = \"grey20\",\n        margin = margin(b = 8)\n      ),\n      plot.subtitle = element_text(\n        size = 14, \n        hjust = 0.5, \n        color = \"grey40\",\n        margin = margin(b = 20)\n      ),\n      \n      # Ejes coherentes\n      axis.title.x = element_text(\n        size = 15, \n        color = \"grey30\", \n        margin = margin(t = 10)\n      ),\n      axis.title.y = element_text(\n        size = 15, \n        color = \"grey30\", \n        margin = margin(r = 10)\n      ),\n      axis.text = element_text(\n        size = 15, \n        color = \"grey40\"\n      ),\n      axis.ticks = element_line(color = \"grey50\", linewidth = 0.4),\n      \n      # Leyenda coherente\n      legend.position = \"bottom\",\n      legend.background = element_rect(fill = \"white\", color = NA),\n      legend.text = element_text(size = 14, color = \"black\"),\n      legend.title = element_text(size = 12, face = \"bold\", color = \"black\"),\n      legend.key.size = unit(1, \"cm\"),\n      legend.margin = margin(t = 15),\n      \n      # Márgenes coherentes\n      plot.margin = margin(20, 20, 15, 20)\n    ) +\n    \n    # Guías de leyenda coherentes\n    guides(\n      fill = guide_legend(\n        title.position = \"left\",\n        nrow = 1,\n        keywidth = unit(1.2, \"cm\"),\n        keyheight = unit(0.8, \"cm\")\n      )\n    )\n  \n  return(p)\n}\n\n# Crear el gráfico con estilo coherente\nfig_coherent_bars <- create_descriptive_coherent_style(generalized_data)\nprint(fig_coherent_bars)\n```\n\n::: {.cell-output-display}\n![](01_model_files/figure-html/plot-functions-1.png){width=3600}\n:::\n:::\n\n\n\n\n\n# Model Specification and Estimation\n\n## Piecewise Dual Process Models\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Formula for the standard linear piecewise model\nformula_piecewise <- diffusion ~ d_ij + delta_up + delta_down + wage_gap\n```\n:::\n\n\n\n\n## Separate Model Estimation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate separate models for each skill type\nmodel_cognitive_piecewise <- glm(formula_piecewise, data = cognitive_data, family = binomial(link = \"logit\"))\nmodel_physical_piecewise <- glm(formula_piecewise, data = physical_data, family = binomial(link = \"logit\"))\n```\n:::\n\n\n\n\n## Parameter Extraction and Comparison\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to extract parameters from GLM models\nextract_piecewise_parameters <- function(model, skill_type, data_subset) {\n  coefs <- coef(model)\n \n  get_param <- function(name, default_val = 0) if (name %in% names(coefs)) coefs[name] else default_val\n\n  return(list(\n    skill_type = skill_type,\n    n_obs = nrow(data_subset),\n    lambda_hat = get_param(\"d_ij\"),              \n    beta_up_hat = get_param(\"delta_up\"),         \n    beta_down_hat = get_param(\"delta_down\"),     \n    wage_hat = get_param(\"wage_gap\"),            \n    asymmetry = get_param(\"delta_up\") - get_param(\"delta_down\"),  \n    aic = AIC(model),\n    mcfadden_r2 = 1 - (model$deviance / model$null.deviance)\n  ))\n}\n\nparams_cognitive_pw <- extract_piecewise_parameters(model_cognitive_piecewise, \"Socio-cognitive\", cognitive_data)\nparams_physical_pw <- extract_piecewise_parameters(model_physical_piecewise, \"Socio-technical\", physical_data)\n```\n:::\n\n\n\n\n# Statistical Tests and Model Comparison\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a comparison table for the piecewise models\npiecewise_comparison <- data.frame(\n  `Skill Type` = c(\"Socio-cognitive\", \"Socio-technical\"),\n  N = c(comma(params_cognitive_pw$n_obs), comma(params_physical_pw$n_obs)),\n  `λ (Distance)` = c(params_cognitive_pw$lambda_hat, params_physical_pw$lambda_hat),\n  `β⁺ (Upward)` = c(params_cognitive_pw$beta_up_hat, params_physical_pw$beta_up_hat),\n  `β⁻ (Downward)` = c(params_cognitive_pw$beta_down_hat, params_physical_pw$beta_down_hat),\n  `Asymmetry (β⁺ - β⁻)` = c(params_cognitive_pw$asymmetry, params_physical_pw$asymmetry),\n  `McFadden's R²` = c(params_cognitive_pw$mcfadden_r2, params_physical_pw$mcfadden_r2),\n  AIC = c(params_cognitive_pw$aic, params_physical_pw$aic),\n  check.names = FALSE\n)\n\nkable(piecewise_comparison,\n      caption = \"**Table 2: Frequentist Piecewise Model Results**\",\n      digits = 4) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = TRUE) %>%\n  column_spec(6, bold = TRUE, background = \"#F0F8FF\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>**Table 2: Frequentist Piecewise Model Results**</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Skill Type </th>\n   <th style=\"text-align:left;\"> N </th>\n   <th style=\"text-align:right;\"> λ (Distance) </th>\n   <th style=\"text-align:right;\"> β⁺ (Upward) </th>\n   <th style=\"text-align:right;\"> β⁻ (Downward) </th>\n   <th style=\"text-align:right;\"> Asymmetry (β⁺ - β⁻) </th>\n   <th style=\"text-align:right;\"> McFadden's R² </th>\n   <th style=\"text-align:right;\"> AIC </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Socio-cognitive </td>\n   <td style=\"text-align:left;\"> 353,988 </td>\n   <td style=\"text-align:right;\"> 1.1178 </td>\n   <td style=\"text-align:right;\"> 0.4440 </td>\n   <td style=\"text-align:right;\"> -0.2582 </td>\n   <td style=\"text-align:right;font-weight: bold;background-color: rgba(240, 248, 255, 255) !important;\"> 0.7022 </td>\n   <td style=\"text-align:right;\"> 0.1206 </td>\n   <td style=\"text-align:right;\"> 362128.6 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Socio-technical </td>\n   <td style=\"text-align:left;\"> 318,951 </td>\n   <td style=\"text-align:right;\"> 2.5693 </td>\n   <td style=\"text-align:right;\"> -0.1216 </td>\n   <td style=\"text-align:right;\"> -0.2251 </td>\n   <td style=\"text-align:right;font-weight: bold;background-color: rgba(240, 248, 255, 255) !important;\"> 0.1035 </td>\n   <td style=\"text-align:right;\"> 0.0129 </td>\n   <td style=\"text-align:right;\"> 367436.0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n**Table 2 Interpretation:** The frequentist models confirm the descriptive findings. Both skill types are penalized by structural distance (negative λ). However, the key result is in the **Asymmetry** column. Socio-cognitive skills have a large, positive asymmetry coefficient (0.7022), indicating that upward status moves are strongly favored over downward moves. In contrast, socio-technical skills have a much smaller asymmetry value (0.1035), suggesting a more neutral response to the direction of status change.\n\n## Model Predictions & Visualization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_piecewise_predictions <- function(model_cog, model_phys, data_cog, data_phys, n_points = 150) {\n \n  baseline_distance_cog <- median(data_cog$d_ij, na.rm = TRUE)\n  baseline_wage_cog <- median(data_cog$wage_gap, na.rm = TRUE)\n\n  baseline_distance_phys <- median(data_phys$d_ij, na.rm = TRUE)\n  baseline_wage_phys <- median(data_phys$wage_gap, na.rm = TRUE)\n \n  # Create status range for predictions\n  status_range <- seq(-4, 4, length.out = n_points)\n \n  # Convert to piecewise variables\n  delta_up_range <- pmax(0, status_range)\n  delta_down_range <- pmax(0, -status_range)\n \n  pred_data_cognitive <- data.frame(\n    delta_up = delta_up_range,\n    delta_down = delta_down_range,\n    d_ij = baseline_distance_cog,\n    wage_gap = baseline_wage_cog\n  )\n  pred_cognitive <- predict(model_cog, newdata = pred_data_cognitive, type = \"response\")\n \n  pred_data_physical <- data.frame(\n    delta_up = delta_up_range,\n    delta_down = delta_down_range,\n    d_ij = baseline_distance_phys,\n    wage_gap = baseline_wage_phys\n  )\n  pred_physical <- predict(model_phys, newdata = pred_data_physical, type = \"response\")\n \n  predictions <- rbind(\n    data.frame(status_gap = status_range, probability = pred_cognitive, content_type = \"Socio-cognitive\"),\n    data.frame(status_gap = status_range, probability = pred_physical, content_type = \"Socio-technical\")\n  )\n \n  return(predictions)\n}\n\n# Generate predictions from the frequentist models\npiecewise_predictions <- generate_piecewise_predictions(model_cognitive_piecewise, model_physical_piecewise, cognitive_data, physical_data)\n\n# Define the equation string for the plot subtitle\nequation_str <- \"logit(P) = &theta;<sub>0</sub> - &lambda;d - &beta;<sup>+</sup>&Delta;<sup>+</sup> - &beta;<sup>-</sup>&Delta;<sup>-</sup>\"\n\np_main_piecewise <- ggplot(piecewise_predictions, aes(x = status_gap, y = probability, color = content_type)) +\n  geom_line(linewidth = 1.8, alpha = 0.9) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.7, color = \"gray50\") +\n  scale_color_manual(values = color_palette, name = \"Skill Type:\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  annotate(\"text\", x = 2.5, y = max(piecewise_predictions$probability) * 0.85, \n           label = \"ASPIRATIONAL PULL\\n(Low-to-High Status)\", \n           hjust = 0.5, size = 4.5, color = \"gray30\", fontface = \"bold\") +\n  annotate(\"text\", x = -2.5, y = max(piecewise_predictions$probability) * 0.85, \n           label = \"CONTAINMENT\\n(High-to-Low Status)\", \n           hjust = 0.5, size = 4.5, color = \"gray30\", fontface = \"bold\") +\n  labs(\n    title = \"Asymmetric Trajectory Channeling: Evidence from Piecewise Models\",\n    subtitle = paste(\"Model:\", equation_str),\n    x = \"Educational Status Gap (Destination - Source)\",\n    y = \"Predicted Skill Diffusion Probability\"\n  ) +\n  theme_paper() +\n  theme(\n    legend.position = \"bottom\",\n    plot.subtitle = element_markdown(size = rel(1.2))\n  )\n\nprint(p_main_piecewise)\n```\n\n::: {.cell-output-display}\n![](01_model_files/figure-html/main-visualization-1.png){width=3600}\n:::\n:::\n\n\n\n\n**Figure 3 Interpretation:** This plot visualizes the core finding from the frequentist models. The steep upward slope for socio-cognitive skills (blue) for positive status gaps (x-axis > 0) illustrates a strong \"aspirational pull.\" Conversely, the flatter curve for socio-technical skills (red) demonstrates their relative indifference to status-gaining imitation, highlighting the \"Asymmetric Trajectory Channeling\" effect.\n\n# Advanced Bayesian Analysis (with Curvature Selection)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"🚀 ADVANCED BAYESIAN SETUP - Non-Linear Model with Shrinkage Priors\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n🚀 ADVANCED BAYESIAN SETUP - Non-Linear Model with Shrinkage Priors\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"====================================================================\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n====================================================================\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"OBJECTIVE: Estimate curvature effects using Laplace (Lasso) priors.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOBJECTIVE: Estimate curvature effects using Laplace (Lasso) priors.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"METHOD: High-quality Hamiltonian Monte Carlo (HMC) on a large sample.\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMETHOD: High-quality Hamiltonian Monte Carlo (HMC) on a large sample.\n```\n\n\n:::\n\n```{.r .cell-code}\n# 1. Subsample data for a balance of speed and robustness\nset.seed(123)\nsample_size <- 50000\n\ncognitive_data_sample <- if (nrow(cognitive_data) > sample_size) {\n  cognitive_data[sample(.N, sample_size)]\n} else {\n  cognitive_data\n}\n\nphysical_data_sample <- if (nrow(physical_data) > sample_size) {\n  physical_data[sample(.N, sample_size)]\n} else {\n  physical_data\n}\n\nmessage(sprintf(\"Subsampling for HMC. Using %s cognitive and %s physical observations.\",\n                comma(nrow(cognitive_data_sample)),\n                comma(nrow(physical_data_sample))))\n\n# 2. Prepare polynomial terms\ncognitive_data_sample <- cognitive_data_sample %>%\n  mutate(\n    delta_up2   = delta_up^2,\n    delta_down2 = delta_down^2\n  )\n\nphysical_data_sample <- physical_data_sample %>%\n  mutate(\n    delta_up2   = delta_up^2,\n    delta_down2 = delta_down^2\n  )\n\n# 3. Non-linear model formula (no underscores in parameter names)\nformula_nonlinear <- bf(\n  diffusion ~ theta0 - lambda * d_ij - beta1up * delta_up - beta2up * delta_up2 - beta1down * delta_down - beta2down * delta_down2,\n  theta0 + lambda + beta1up + beta2up + beta1down + beta2down ~ 1,\n  nl = TRUE\n)\n\n# 4. Prior specification (using double_exponential)\npriors_lasso <- c(\n  prior(normal(0, 5), nlpar = \"theta0\"),\n  prior(normal(0, 1), nlpar = \"lambda\", lb = 0), \n  prior(normal(0, 1), nlpar = \"beta1up\"),\n  prior(normal(0, 1), nlpar = \"beta1down\"),\n  prior(double_exponential(0, 0.5), nlpar = \"beta2up\"),\n  prior(double_exponential(0, 0.5), nlpar = \"beta2down\")\n)\n\nstart_hmc_time <- Sys.time()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# =============================================================================\n# HIGH-QUALITY FITTING FUNCTION FOR ADVANCED MODEL USING HMC\n# =============================================================================\n\nfit_advanced_hmc_model <- function(formula, data, model_name, priors) {\n \n  cat(sprintf(\"\\n🚀 ADVANCED FITTING (HMC): %s\\n\", toupper(model_name)))\n  cat(\"=\" %>% rep(50) %>% paste(collapse = \"\"), \"\\n\")\n  cat(sprintf(\"📊 Using %s observations from sample.\\n\", comma(nrow(data))))\n  cat(\"⚙️ Algorithm: Hamiltonian Monte Carlo (NUTS Sampler).\\n\")\n \n  start_time <- Sys.time()\n  model <- NULL\n \n  tryCatch({\n    model <- brm(\n      formula = formula,\n      data = data,\n      prior = priors,\n      family = bernoulli(),\n      chains = 2,\n      cores = 4,\n      iter = 1000,\n      warmup = 500,\n      seed = 42,\n      silent = 0,\n      control = list(adapt_delta = 0.9)\n    )\n    cat(\"✅ Advanced HMC model fitted successfully.\\n\")\n  }, error = function(e) {\n    cat(\"❌ ERROR during advanced model fitting:\\n\")\n    cat(e$message, \"\\n\")\n  })\n \n  end_time <- Sys.time()\n  time_elapsed <- difftime(end_time, start_time, units = \"secs\")\n  cat(sprintf(\"⏱️ Fitting time: %.1f seconds.\\n\", time_elapsed))\n \n  return(list(model = model, time = time_elapsed))\n}\n\n# =============================================================================\n# FIT MODELS WITH ADVANCED CONFIGURATION\n# =============================================================================\n\n# ADVANCED COGNITIVE MODEL\nresults_cognitive_advanced <- fit_advanced_hmc_model(\n  formula = formula_nonlinear,\n  data = cognitive_data_sample,\n  model_name = \"cognitive_advanced_hmc\",\n  priors = priors_lasso\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n🚀 ADVANCED FITTING (HMC): COGNITIVE_ADVANCED_HMC\n================================================== \n📊 Using 50,000 observations from sample.\n⚙️ Algorithm: Hamiltonian Monte Carlo (NUTS Sampler).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✅ Advanced HMC model fitted successfully.\n⏱️ Fitting time: 2329.6 seconds.\nation took 0.19568 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1956.8 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 2: \nChain 2: Gradient evaluation took 0.256456 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 2564.56 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 1: Iteration:   1 / 1000 [  0%]  (Warmup)\nChain 2: Iteration:   1 / 1000 [  0%]  (Warmup)\nChain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)\nChain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)\nChain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)\nChain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)\nChain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)\nChain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)\nChain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)\nChain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)\nChain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)\nChain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)\nChain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)\nChain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)\nChain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)\nChain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)\nChain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)\nChain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)\nChain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)\nChain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)\nChain 1: Iteration: 1000 / 1000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1158.59 seconds (Warm-up)\nChain 1:                872.174 seconds (Sampling)\nChain 1:                2030.76 seconds (Total)\nChain 1: \nChain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)\nChain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)\nChain 2: Iteration: 1000 / 1000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1526.69 seconds (Warm-up)\nChain 2:                721.66 seconds (Sampling)\nChain 2:                2248.35 seconds (Total)\nChain 2: \n```\n\n\n:::\n\n```{.r .cell-code}\n# ADVANCED PHYSICAL MODEL\nresults_physical_advanced <- fit_advanced_hmc_model(\n  formula = formula_nonlinear,\n  data = physical_data_sample,\n  model_name = \"physical_advanced_hmc\",\n  priors = priors_lasso\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80%]  (Sampling)\nChain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)\nChain 2: Iteration: 1000 / 1000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1162.02 seconds (Warm-up)\nChain 2:                551.458 seconds (Sampling)\nChain 2:                1713.47 seconds (Total)\nChain 2: \n```\n\n\n:::\n\n```{.r .cell-code}\nend_hmc_time <- Sys.time()\ntotal_analysis_time <- difftime(end_hmc_time, start_hmc_time, units = \"secs\")\n\ncat(\"\\n🏁 ADVANCED ANALYSIS COMPLETE\\n\")\ncat(\"=====================================\\n\")\ncat(sprintf(\"⏱️ Total analysis time: %.1f seconds\\n\", total_analysis_time))\n\nall_success_advanced <- !is.null(results_cognitive_advanced$model) && !is.null(results_physical_advanced$model)\n\nif(all_success_advanced) {\n  cat(\"✅ Both advanced models fitted successfully.\\n\")\n} else {\n  cat(\"❌ Error in fitting one or more advanced models.\\n\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# =============================================================================\n# PARAMETER EXTRACTION (ADVANCED MODEL)\n# =============================================================================\n\nif(all_success_advanced) {\n  cat(\"\\n🔍 EXTRACTING PARAMETERS FROM ADVANCED MODEL...\\n\")\n  cat(\"=\" %>% rep(55) %>% paste(collapse = \"\"), \"\\n\")\n \n  # Extract posterior draws\n  posterior_cog_advanced <- as_draws_df(results_cognitive_advanced$model)\n  posterior_phys_advanced <- as_draws_df(results_physical_advanced$model)\n \n  # Correct parameter names based on brms output for non-linear models\n  required_cols <- c(\"b_beta1up_Intercept\", \"b_beta1down_Intercept\")\n  \n  if (all(required_cols %in% names(posterior_cog_advanced)) && all(required_cols %in% names(posterior_phys_advanced))) {\n      # Asymmetry parameters (based on linear terms)\n      asymmetry_cog_advanced <- posterior_cog_advanced$b_beta1up_Intercept - posterior_cog_advanced$b_beta1down_Intercept\n      asymmetry_phys_advanced <- posterior_phys_advanced$b_beta1up_Intercept - posterior_phys_advanced$b_beta1down_Intercept\n      asymmetry_diff_advanced <- asymmetry_cog_advanced - asymmetry_phys_advanced\n     \n      # Statistics\n      prob_hypothesis_advanced <- mean(asymmetry_diff_advanced > 0, na.rm = TRUE)\n      effect_size_advanced <- median(asymmetry_diff_advanced, na.rm = TRUE)\n      ci_95_advanced <- quantile(asymmetry_diff_advanced, c(0.025, 0.975), na.rm = TRUE)\n      \n      cat(\"🎯 PRELIMINARY RESULTS (ADVANCED MODEL):\\n\")\n      cat(\"=\" %>% rep(45) %>% paste(collapse = \"\"), \"\\n\")\n      cat(sprintf(\"• P(asymmetry_cog > asymmetry_phys): %.2f%%\\n\", prob_hypothesis_advanced * 100))\n      cat(sprintf(\"• Median Asymmetry Difference: %.4f\\n\", effect_size_advanced))\n      cat(sprintf(\"• 95%% Credible Interval (approx.): [%.4f, %.4f]\\n\", ci_95_advanced[1], ci_95_advanced[2]))\n  } else {\n      # Set to NA if parameters were not found\n      prob_hypothesis_advanced <- NA\n      effect_size_advanced <- NA\n      ci_95_advanced <- c(NA, NA)\n      cat(\"⚠️ WARNING: Expected parameters not found in model output.\\n\")\n  }\n \n  cat(\"\\n✅ ADVANCED PARAMETER EXTRACTION COMPLETE\\n\")\n \n} else {\n  cat(\"\\n❌ CANNOT EXTRACT PARAMETERS DUE TO MODEL FAILURE\\n\")\n  # Assign NA so the next chunk doesn't fail\n  prob_hypothesis_advanced <- NA \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n🔍 EXTRACTING PARAMETERS FROM ADVANCED MODEL...\n======================================================= \n🎯 PRELIMINARY RESULTS (ADVANCED MODEL):\n============================================= \n• P(asymmetry_cog > asymmetry_phys): 0.00%\n• Median Asymmetry Difference: -0.5865\n• 95% Credible Interval (approx.): [-0.6923, -0.4836]\n\n✅ ADVANCED PARAMETER EXTRACTION COMPLETE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# =============================================================================\n# BAYESIAN PREDICTIONS (ADVANCED MODEL)\n# =============================================================================\n\nif(exists(\"all_success_advanced\") && all_success_advanced) {\n  cat(\"\\n🔮 GENERATING BAYESIAN PREDICTIONS (ADVANCED MODEL)...\\n\")\n  cat(\"=\" %>% rep(60) %>% paste(collapse = \"\"), \"\\n\")\n \n  generate_advanced_bayesian_predictions <- function(model_cog, model_phys, n_points = 100) {\n   \n    status_range <- seq(-4, 4, length.out = n_points)\n    delta_up_range <- pmax(0, status_range)\n    delta_down_range <- pmax(0, -status_range)\n   \n    # Must include quadratic terms in prediction data\n    newdata_cog <- data.frame(\n      delta_up = delta_up_range,\n      delta_down = delta_down_range,\n      delta_up2 = delta_up_range^2,\n      delta_down2 = delta_down_range^2,\n      d_ij = median(cognitive_data_sample$d_ij, na.rm = TRUE)\n    )\n   \n    newdata_phys <- data.frame(\n      delta_up = delta_up_range,\n      delta_down = delta_down_range,\n      delta_up2 = delta_up_range^2,\n      delta_down2 = delta_down_range^2,\n      d_ij = median(physical_data_sample$d_ij, na.rm = TRUE)\n    )\n   \n    pred_cog <- fitted(model_cog, newdata = newdata_cog, summary = TRUE)\n    pred_phys <- fitted(model_phys, newdata = newdata_phys, summary = TRUE)\n   \n    predictions <- rbind(\n      data.frame(\n        status_gap = status_range,\n        probability = pred_cog[, \"Estimate\"],\n        lower_95 = pred_cog[, \"Q2.5\"],\n        upper_95 = pred_cog[, \"Q97.5\"],\n        content_type = \"Socio-cognitive\"\n      ),\n      data.frame(\n        status_gap = status_range,\n        probability = pred_phys[, \"Estimate\"],\n        lower_95 = pred_phys[, \"Q2.5\"],\n        upper_95 = pred_phys[, \"Q97.5\"],\n        content_type = \"Socio-technical\"\n      )\n    )\n   \n    return(predictions)\n  }\n \n  bayesian_advanced_predictions <- generate_advanced_bayesian_predictions(\n    results_cognitive_advanced$model, \n    results_physical_advanced$model\n  )\n \n  cat(\"✅ Advanced model predictions generated.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n🔮 GENERATING BAYESIAN PREDICTIONS (ADVANCED MODEL)...\n============================================================ \n✅ Advanced model predictions generated.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# =============================================================================\n# BAYESIAN VISUALIZATION (ADVANCED MODEL)\n# =============================================================================\n\nif(exists(\"all_success_advanced\") && all_success_advanced && exists(\"bayesian_advanced_predictions\")) {\n \n  # Safely handle NA values so the plot doesn't fail\n  if (is.na(prob_hypothesis_advanced)) {\n    evidence_label_advanced <- \"INDETERMINATE\"\n    caption_text <- \"95% Credible Interval (HMC). Model results were indeterminate.\"\n  } else {\n    evidence_label_advanced <- if(prob_hypothesis_advanced > 0.99) \"STRONG\" \n                               else if(prob_hypothesis_advanced > 0.95) \"MODERATE\"\n                               else \"WEAK\"\n    caption_text <- sprintf(\"95%% Credible Interval (HMC). Evidence for linear asymmetry: **%s** (P = %.1f%%)\", \n                            evidence_label_advanced, prob_hypothesis_advanced * 100)\n  }\n  \n  equation_str_adv <- \"logit(P) = &theta;<sub>0</sub> - &lambda;d - &beta;<sub>1</sub><sup>+</sup>&Delta;<sup>+</sup> - &beta;<sub>2</sub><sup>+</sup>(&Delta;<sup>+</sup>)<sup>2</sup> - &beta;<sub>1</sub><sup>-</sup>&Delta;<sup>-</sup> - &beta;<sub>2</sub><sup>-</sup>(&Delta;<sup>-</sup>)<sup>2</sup>\"\n\n\n  p_bayesian_advanced <- ggplot(bayesian_advanced_predictions, \n                                aes(x = status_gap, color = content_type, fill = content_type)) +\n   \n    geom_ribbon(aes(ymin = lower_95, ymax = upper_95), alpha = 0.2, color = NA) +\n    geom_line(aes(y = probability), linewidth = 2, alpha = 0.9) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.8, color = \"gray40\", linewidth = 1) +\n    scale_color_manual(values = color_palette, name = \"Skill Type:\") +\n    scale_fill_manual(values = color_palette, name = \"Skill Type:\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_x_continuous(breaks = seq(-4, 4, 2)) +\n   \n    labs(\n      title = \"Advanced Bayesian Analysis: Curvature Effects\",\n      subtitle = paste(\"Model:\", equation_str_adv),\n      x = \"Educational Status Gap (Destination - Source)\",\n      y = \"Predicted Skill Diffusion Probability\",\n      caption = caption_text\n    ) +\n   \n    theme_paper(base_size = 16) +\n    theme(\n      plot.title = element_text(size = rel(1.4)),\n      plot.subtitle = element_markdown(size = rel(1.2)),\n      plot.caption = element_markdown(size = rel(0.9)),\n      legend.position = \"bottom\"\n    )\n \n  print(p_bayesian_advanced)\n \n  ggsave(file.path(output_dir, \"bayesian_advanced_hmc_channeling.png\"), \n         p_bayesian_advanced, width = 12, height = 8, dpi = 300, bg = \"white\")\n \n  cat(\"\\n✅ ADVANCED VISUALIZATION COMPLETE\\n\")\n \n} else {\n  cat(\"\\n❌ COULD NOT GENERATE ADVANCED VISUALIZATION\\n\")\n  cat(\"Reason: Advanced Bayesian analysis did not complete successfully.\\n\")\n}\n```\n\n::: {.cell-output-display}\n![](01_model_files/figure-html/bayesian-visualization-advanced-1.png){width=3600}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n✅ ADVANCED VISUALIZATION COMPLETE\n```\n\n\n:::\n:::\n\n\n\n\n# Fixed Effects Causal Analysis: Controlling for Unobserved Heterogeneity\n\nThe previous analyses have demonstrated strong evidence for Asymmetric Trajectory Channeling through piecewise models and Bayesian approaches. However, a potential concern is that unobserved characteristics of source occupations might confound our results. To address this concern and strengthen our causal identification, we now implement a fixed effects strategy that controls for all time-invariant unobserved heterogeneity at the source occupation level.\n\n## Fixed Effects Model Specification\n\nOur fixed effects specification builds directly on the theoretical piecewise model but includes source occupation fixed effects ($\\alpha_i$):\n\n$$\n\\text{logit}\\, P_{i \\to j}^{(c)} = \\alpha_i + \\theta_{0c} - \\lambda_c d_{ij} - \\beta_c^+ \\Delta^+_{ij} - \\beta_c^- \\Delta^-_{ij} - \\omega_c w_{ij}\n$$\n\nWhere $\\alpha_i$ captures all time-invariant characteristics of source occupation $i$ that might affect its propensity to be imitated. This specification allows us to identify the causal effects of structural distance and status differences from within-source-occupation variation, effectively comparing how the same source occupation's skills diffuse to different target occupations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data for fixed effects analysis\n# Add fixed effects variables and ensure same distance variable is used\ngeneralized_data[, `:=`(\n  # Outcome variable\n  diffusion_outcome = as.numeric(diffusion),\n  \n  # Main explanatory variables (using distance_euclidean_rca consistently)\n  structural_distance = d_ij,  # This is already distance_euclidean_rca from earlier\n  education_diff = status_gap_signed,  # Use the signed version for fixed effects\n  wage_diff = abs(wage_gap),\n  \n  # Fixed effects variables\n  source_fe = as.factor(source),\n  target_fe = as.factor(target)\n)]\n\n# Add skill type indicator (consistent with earlier analysis)\nif (\"LeidenCluster_2015\" %in% names(generalized_data)) {\n  generalized_data[, cognitive_skills := as.numeric(LeidenCluster_2015 == \"LeidenC_1\")]\n} else {\n  # Fallback based on content_type\n  generalized_data[, cognitive_skills := as.numeric(content_type == \"Socio-cognitive\")]\n}\n\n# Clean data for fixed effects estimation\nfe_data <- generalized_data[!is.na(diffusion_outcome) & \n                           !is.na(structural_distance) & \n                           !is.na(education_diff) & \n                           !is.na(wage_diff) &\n                           is.finite(structural_distance) &\n                           is.finite(education_diff) &\n                           is.finite(wage_diff)]\n\n# Split by skill type\nfe_cognitive <- fe_data[cognitive_skills == 1]\nfe_physical <- fe_data[cognitive_skills == 0]\n\nmessage(sprintf(\"Fixed Effects Data: %s total observations\", comma(nrow(fe_data))))\nmessage(sprintf(\"  - Cognitive skills: %s observations\", comma(nrow(fe_cognitive))))\nmessage(sprintf(\"  - Physical skills: %s observations\", comma(nrow(fe_physical))))\nmessage(sprintf(\"  - Using %s as structural distance variable\", distance_var_used))\n```\n:::\n\n\n\n\n## Fixed Effects Model Estimation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate fixed effects models using feols from fixest package\n# Formula consistent with theoretical specification\nfe_formula <- diffusion_outcome ~ structural_distance + education_diff + wage_diff | source_fe\n\n# Estimate models for each skill type\nmessage(\"Estimating fixed effects models...\")\n\n# Cognitive skills model\nfe_model_cognitive <- feols(fe_formula, \n                           data = fe_cognitive,\n                           cluster = \"source_fe\")\n\n# Physical skills model  \nfe_model_physical <- feols(fe_formula, \n                          data = fe_physical,\n                          cluster = \"source_fe\")\n\nmessage(\"Fixed effects models estimated successfully\")\n\n# Extract key coefficients\ncoef_cog_struct <- coef(fe_model_cognitive)[\"structural_distance\"]\ncoef_phys_struct <- coef(fe_model_physical)[\"structural_distance\"]\n\n# Display immediate verification of signs\nmessage(sprintf(\"\\nStructural Distance Coefficients:\"))\nmessage(sprintf(\"  Cognitive: %.4f %s\", \n                coef_cog_struct, \n                ifelse(coef_cog_struct < 0, \"(✓ Negative - matches theory)\", \"(⚠ Positive - check specification)\")))\nmessage(sprintf(\"  Physical:  %.4f %s\", \n                coef_phys_struct, \n                ifelse(coef_phys_struct < 0, \"(✓ Negative - matches theory)\", \"(⚠ Positive - check specification)\")))\n```\n:::\n\n\n\n\n## Fixed Effects Results and Significance Testing\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to test coefficient differences between models\ntest_coefficient_difference <- function(model1, model2, coef_name) {\n  b1 <- coef(model1)[coef_name]\n  b2 <- coef(model2)[coef_name]\n  se1 <- se(model1)[coef_name]\n  se2 <- se(model2)[coef_name]\n  \n  if(is.na(b1) || is.na(b2) || is.na(se1) || is.na(se2)) {\n    return(list(diff = NA, z_stat = NA, p_value = NA))\n  }\n  \n  diff <- b1 - b2\n  se_diff <- sqrt(se1^2 + se2^2)\n  z_stat <- diff / se_diff\n  p_value <- 2 * pnorm(-abs(z_stat))\n  \n  return(list(diff = diff, z_stat = z_stat, p_value = p_value))\n}\n\n# Test differences for key coefficients\ndiff_structural <- test_coefficient_difference(fe_model_cognitive, fe_model_physical, \"structural_distance\")\ndiff_education <- test_coefficient_difference(fe_model_cognitive, fe_model_physical, \"education_diff\")\ndiff_wage <- test_coefficient_difference(fe_model_cognitive, fe_model_physical, \"wage_diff\")\n\n# Extract results safely and create simple table\ncreate_fe_results_table <- function(model_cog, model_phys, diff_tests) {\n  \n  # Extract coefficients safely\n  get_coef_safe <- function(model, name) {\n    coefs <- coef(model)\n    if(name %in% names(coefs)) coefs[name] else NA\n  }\n  \n  get_se_safe <- function(model, name) {\n    ses <- se(model)\n    if(name %in% names(ses)) ses[name] else NA\n  }\n  \n  get_p_safe <- function(model, name) {\n    ps <- pvalue(model)\n    if(name %in% names(ps)) ps[name] else NA\n  }\n  \n  # Create results data frame\n  results_df <- data.frame(\n    Variable = c(\"Structural Distance\", \"Education Difference\", \"Wage Difference\"),\n    \n    # Cognitive results\n    Cog_Coef = c(\n      get_coef_safe(model_cog, \"structural_distance\"),\n      get_coef_safe(model_cog, \"education_diff\"),\n      get_coef_safe(model_cog, \"wage_diff\")\n    ),\n    Cog_SE = c(\n      get_se_safe(model_cog, \"structural_distance\"),\n      get_se_safe(model_cog, \"education_diff\"),\n      get_se_safe(model_cog, \"wage_diff\")\n    ),\n    Cog_P = c(\n      get_p_safe(model_cog, \"structural_distance\"),\n      get_p_safe(model_cog, \"education_diff\"),\n      get_p_safe(model_cog, \"wage_diff\")\n    ),\n    \n    # Physical results\n    Phys_Coef = c(\n      get_coef_safe(model_phys, \"structural_distance\"),\n      get_coef_safe(model_phys, \"education_diff\"),\n      get_coef_safe(model_phys, \"wage_diff\")\n    ),\n    Phys_SE = c(\n      get_se_safe(model_phys, \"structural_distance\"),\n      get_se_safe(model_phys, \"education_diff\"),\n      get_se_safe(model_phys, \"wage_diff\")\n    ),\n    Phys_P = c(\n      get_p_safe(model_phys, \"structural_distance\"),\n      get_p_safe(model_phys, \"education_diff\"),\n      get_p_safe(model_phys, \"wage_diff\")\n    ),\n    \n    # Differences\n    Diff = c(diff_tests$structural$diff, diff_tests$education$diff, diff_tests$wage$diff),\n    Diff_P = c(diff_tests$structural$p_value, diff_tests$education$p_value, diff_tests$wage$p_value),\n    \n    stringsAsFactors = FALSE\n  )\n  \n  # Format for display\n  results_df$Cognitive_Skills <- sprintf(\"%.4f%s\\n(%.4f)\",\n                                         results_df$Cog_Coef,\n                                         ifelse(results_df$Cog_P < 0.001, \"***\",\n                                               ifelse(results_df$Cog_P < 0.01, \"**\",\n                                                     ifelse(results_df$Cog_P < 0.05, \"*\", \"\"))),\n                                         results_df$Cog_SE)\n  \n  results_df$Physical_Skills <- sprintf(\"%.4f%s\\n(%.4f)\",\n                                        results_df$Phys_Coef,\n                                        ifelse(results_df$Phys_P < 0.001, \"***\",\n                                              ifelse(results_df$Phys_P < 0.01, \"**\",\n                                                    ifelse(results_df$Phys_P < 0.05, \"*\", \"\"))),\n                                        results_df$Phys_SE)\n  \n  results_df$Difference <- sprintf(\"%.4f%s\",\n                                   results_df$Diff,\n                                   ifelse(results_df$Diff_P < 0.05, \"*\", \"\"))\n  \n  # Return clean table\n  return(results_df[, c(\"Variable\", \"Cognitive_Skills\", \"Physical_Skills\", \"Difference\")])\n}\n\n# Create the table\ndiff_tests <- list(\n  structural = diff_structural,\n  education = diff_education,\n  wage = diff_wage\n)\n\nfe_table_clean <- create_fe_results_table(fe_model_cognitive, fe_model_physical, diff_tests)\n\n# Display with kable\nkable(fe_table_clean,\n      caption = \"**Table 3: Fixed Effects Regression Results - Asymmetric Trajectory Channeling**\",\n      col.names = c(\"Variable\", \"Socio-cognitive Skills\", \"Socio-technical Skills\", \"Difference\"),\n      align = c(\"l\", \"c\", \"c\", \"c\"),\n      escape = FALSE) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = TRUE) %>%\n  column_spec(1, bold = TRUE) %>%\n  column_spec(4, bold = TRUE, background = \"#F0F8FF\") %>%\n  footnote(general = sprintf(\"Standard errors in parentheses. N(cognitive)=%s, N(physical)=%s. Distance variable: %s. Source occupation fixed effects included. ***p<0.001, **p<0.01, *p<0.05\", \n                            comma(nrow(fe_cognitive)), comma(nrow(fe_physical)), distance_var_used),\n           general_title = \"Note:\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;border-bottom: 0;\">\n<caption>**Table 3: Fixed Effects Regression Results - Asymmetric Trajectory Channeling**</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:center;\"> Socio-cognitive Skills </th>\n   <th style=\"text-align:center;\"> Socio-technical Skills </th>\n   <th style=\"text-align:center;\"> Difference </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> structural_distance </td>\n   <td style=\"text-align:left;\"> Structural Distance </td>\n   <td style=\"text-align:center;\"> 0.3737***\n(0.0649) </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: rgba(240, 248, 255, 255) !important;\"> |   0.4439***\n(0.0706) </td>\n   <td style=\"text-align:center;\"> |  -0.0701 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> education_diff </td>\n   <td style=\"text-align:left;\"> Education Difference </td>\n   <td style=\"text-align:center;\"> 0.1116***\n(0.0022) </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: rgba(240, 248, 255, 255) !important;\"> |   -0.0081***\n(0.0023) </td>\n   <td style=\"text-align:center;\"> |  0.1197* </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> wage_diff </td>\n   <td style=\"text-align:left;\"> Wage Difference </td>\n   <td style=\"text-align:center;\"> -0.0000\n(0.0000) </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: rgba(240, 248, 255, 255) !important;\"> |   -0.0000***\n(0.0000) </td>\n   <td style=\"text-align:center;\"> |  0.0000* </td>\n  </tr>\n</tbody>\n<tfoot>\n<tr><td style=\"padding: 0; \" colspan=\"100%\"><span style=\"font-style: italic;\">Note:</span></td></tr>\n<tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> Standard errors in parentheses. N(cognitive)=353,988, N(physical)=318,951. Distance variable: structural_distance. Source occupation fixed effects included. ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05</td></tr>\n</tfoot>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Model summary statistics (corrected for vector length issues)\nget_r2_safe <- function(model) {\n  # Try multiple ways to extract R² and ensure single value\n  r2_val <- tryCatch({\n    if(\"r2_within\" %in% names(model) && !is.null(model$r2_within)) {\n      as.numeric(model$r2_within[1])\n    } else if(\"adj_r2\" %in% names(model) && !is.null(model$adj_r2)) {\n      as.numeric(model$adj_r2[1])\n    } else if(hasName(model, \"r.squared\")) {\n      as.numeric(model$r.squared[1])\n    } else {\n      # Calculate manually if possible\n      summary_model <- summary(model)\n      if(\"r.squared\" %in% names(summary_model)) {\n        as.numeric(summary_model$r.squared[1])\n      } else {\n        NA_real_\n      }\n    }\n  }, error = function(e) {\n    NA_real_\n  })\n  \n  # Ensure single value\n  if(length(r2_val) > 1) r2_val <- r2_val[1]\n  if(is.null(r2_val)) r2_val <- NA_real_\n  \n  return(r2_val)\n}\n\n# Safe extraction functions to ensure single values\nget_nobs_safe <- function(model) {\n  n <- tryCatch({\n    as.numeric(nobs(model)[1])\n  }, error = function(e) {\n    nrow(model$data)\n  })\n  return(n)\n}\n\nget_nfe_safe <- function(data) {\n  n <- tryCatch({\n    as.numeric(length(unique(data$source_fe))[1])\n  }, error = function(e) {\n    1L\n  })\n  return(n)\n}\n\n# Extract each value safely\nn_obs_cog <- get_nobs_safe(fe_model_cognitive)\nn_obs_phys <- get_nobs_safe(fe_model_physical)\nr2_cog <- get_r2_safe(fe_model_cognitive)\nr2_phys <- get_r2_safe(fe_model_physical)\nnfe_cog <- get_nfe_safe(fe_cognitive)\nnfe_phys <- get_nfe_safe(fe_physical)\n\n# Debug: Check lengths\nmessage(sprintf(\"Debug - lengths: n_obs_cog=%d, r2_cog=%d, nfe_cog=%d\", \n                length(n_obs_cog), length(r2_cog), length(nfe_cog)))\n\nfe_summary_clean <- data.frame(\n  Statistic = c(\"Observations\", \"R² Within\", \"Number of Source FE\", \"Distance Variable\"),\n  Cognitive = c(\n    comma(n_obs_cog),\n    sprintf(\"%.4f\", r2_cog),\n    as.character(nfe_cog),\n    distance_var_used\n  ),\n  Physical = c(\n    comma(n_obs_phys), \n    sprintf(\"%.4f\", r2_phys),\n    as.character(nfe_phys),\n    distance_var_used\n  ),\n  stringsAsFactors = FALSE\n)\n\nkable(fe_summary_clean,\n      caption = \"**Table 4: Fixed Effects Model Summary Statistics**\",\n      col.names = c(\"Statistic\", \"Socio-cognitive Model\", \"Socio-technical Model\"),\n      align = c(\"l\", \"c\", \"c\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>**Table 4: Fixed Effects Model Summary Statistics**</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Statistic </th>\n   <th style=\"text-align:center;\"> Socio-cognitive Model </th>\n   <th style=\"text-align:center;\"> Socio-technical Model </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Observations </td>\n   <td style=\"text-align:center;\"> 353,988 </td>\n   <td style=\"text-align:center;\"> 318,951 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R² Within </td>\n   <td style=\"text-align:center;\"> NA </td>\n   <td style=\"text-align:center;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of Source FE </td>\n   <td style=\"text-align:center;\"> 62 </td>\n   <td style=\"text-align:center;\"> 62 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Distance Variable </td>\n   <td style=\"text-align:center;\"> structural_distance </td>\n   <td style=\"text-align:center;\"> structural_distance </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n## Fixed Effects Analysis Interpretation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficients for interpretation (make sure variables are defined in this chunk)\ncoef_cog_struct <- coef(fe_model_cognitive)[\"structural_distance\"]\ncoef_phys_struct <- coef(fe_model_physical)[\"structural_distance\"]\n\n# Generate interpretation based on results\ncat(\"🎯 === FIXED EFFECTS ANALYSIS SUMMARY ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n🎯 === FIXED EFFECTS ANALYSIS SUMMARY ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"==============================================\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n==============================================\n```\n\n\n:::\n\n```{.r .cell-code}\n# Structural distance interpretation\nif(coef_cog_struct < 0 && coef_phys_struct > 0) {\n  cat(\"✅ STRONG SUPPORT FOR ASYMMETRIC TRAJECTORY CHANNELING:\\n\")\n  cat(sprintf(\"   • Cognitive skills: β = %.4f (negative, as predicted)\\n\", coef_cog_struct))\n  cat(sprintf(\"   • Physical skills:  β = %.4f (positive, shows different mechanism)\\n\", coef_phys_struct))\n  cat(\"   • Cognitive skills are MORE constrained by structural distance\\n\")\n  cat(\"   • Physical skills show less structural constraint or even reverse pattern\\n\")\n} else if(coef_cog_struct < 0 && coef_phys_struct < 0) {\n  if(abs(coef_cog_struct) > abs(coef_phys_struct)) {\n    cat(\"✅ MODERATE SUPPORT FOR ASYMMETRIC TRAJECTORY CHANNELING:\\n\")\n    cat(sprintf(\"   • Cognitive skills: β = %.4f (more negative)\\n\", coef_cog_struct))\n    cat(sprintf(\"   • Physical skills:  β = %.4f (less negative)\\n\", coef_phys_struct))\n    cat(\"   • Both constrained by distance, but cognitive skills MORE constrained\\n\")\n  } else {\n    cat(\"⚠️ MIXED RESULTS:\\n\")\n    cat(sprintf(\"   • Cognitive skills: β = %.4f\\n\", coef_cog_struct))\n    cat(sprintf(\"   • Physical skills:  β = %.4f\\n\", coef_phys_struct))\n    cat(\"   • Both show similar structural constraints\\n\")\n  }\n} else {\n  cat(\"❌ UNEXPECTED PATTERN:\\n\")\n  cat(sprintf(\"   • Cognitive skills: β = %.4f\\n\", coef_cog_struct))\n  cat(sprintf(\"   • Physical skills:  β = %.4f\\n\", coef_phys_struct))\n  cat(\"   • Pattern does not match theoretical predictions\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n❌ UNEXPECTED PATTERN:\n   • Cognitive skills: β = 0.3737\n   • Physical skills:  β = 0.4439\n   • Pattern does not match theoretical predictions\n```\n\n\n:::\n\n```{.r .cell-code}\n# Significance of differences\nsignificant_diffs <- sum(c(diff_structural$p_value, diff_education$p_value, diff_wage$p_value) < 0.05, na.rm = TRUE)\ncat(\"\\n📊 STATISTICAL EVIDENCE:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n📊 STATISTICAL EVIDENCE:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • %d of 3 coefficients show significant differences between skill types\\n\", significant_diffs))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • 2 of 3 coefficients show significant differences between skill types\n```\n\n\n:::\n\n```{.r .cell-code}\nif(significant_diffs >= 2) {\n  cat(\"   • STRONG statistical evidence for asymmetric channeling\\n\")\n} else if(significant_diffs == 1) {\n  cat(\"   • MODERATE statistical evidence for asymmetric channeling\\n\")\n} else {\n  cat(\"   • WEAK statistical evidence for asymmetric channeling\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • STRONG statistical evidence for asymmetric channeling\n```\n\n\n:::\n\n```{.r .cell-code}\n# Theory verification\ncat(\"\\n🔬 THEORETICAL VERIFICATION:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n🔬 THEORETICAL VERIFICATION:\n```\n\n\n:::\n\n```{.r .cell-code}\nif(coef_cog_struct < 0) {\n  cat(\"   ✅ Cognitive skills negatively affected by structural distance (supports theory)\\n\")\n} else {\n  cat(\"   ⚠️ Cognitive skills positively affected by structural distance (contradicts theory)\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ⚠️ Cognitive skills positively affected by structural distance (contradicts theory)\n```\n\n\n:::\n\n```{.r .cell-code}\nif(diff_structural$p_value < 0.05) {\n  cat(\"   ✅ Statistically significant difference in distance effects between skill types\\n\")\n} else {\n  cat(\"   ⚠️ No significant difference in distance effects between skill types\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ⚠️ No significant difference in distance effects between skill types\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n🏁 CONCLUSION:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n🏁 CONCLUSION:\n```\n\n\n:::\n\n```{.r .cell-code}\nif(coef_cog_struct < 0 && significant_diffs >= 1) {\n  cat(\"   🏆 Fixed effects analysis CONFIRMS Asymmetric Trajectory Channeling\\n\")\n  cat(\"   📈 Causal identification strengthened by controlling for source occupation heterogeneity\\n\")\n  cat(\"   💡 Results robust to unobserved confounders at the source level\\n\")\n} else {\n  cat(\"   ⚠️ Fixed effects results are mixed or do not support the main hypothesis\\n\")\n  cat(\"   🔍 May require model refinement or alternative specifications\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ⚠️ Fixed effects results are mixed or do not support the main hypothesis\n   🔍 May require model refinement or alternative specifications\n```\n\n\n:::\n:::\n\n\n\n\n## Fixed Effects Visualization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create visualization comparing fixed effects results with earlier analyses\nfe_comparison_data <- data.frame(\n  Model = rep(c(\"Piecewise GLM\", \"Fixed Effects\"), each = 2),\n  Skill_Type = rep(c(\"Socio-cognitive\", \"Socio-technical\"), 2),\n  Structural_Distance_Coef = c(\n    params_cognitive_pw$lambda_hat,  # From earlier piecewise model\n    params_physical_pw$lambda_hat,\n    coef_cog_struct,  # From fixed effects\n    coef_phys_struct\n  ),\n  SE = c(\n    summary(model_cognitive_piecewise)$coefficients[\"d_ij\", \"Std. Error\"],\n    summary(model_physical_piecewise)$coefficients[\"d_ij\", \"Std. Error\"],\n    se(fe_model_cognitive)[\"structural_distance\"],\n    se(fe_model_physical)[\"structural_distance\"]\n  )\n)\n\n# Add confidence intervals\nfe_comparison_data$CI_Lower <- fe_comparison_data$Structural_Distance_Coef - 1.96 * fe_comparison_data$SE\nfe_comparison_data$CI_Upper <- fe_comparison_data$Structural_Distance_Coef + 1.96 * fe_comparison_data$SE\n\n# Create comparison plot\np_fe_comparison <- ggplot(fe_comparison_data, \n                         aes(x = Model, y = Structural_Distance_Coef, \n                             color = Skill_Type, group = Skill_Type)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.7, color = \"gray50\") +\n  geom_line(linewidth = 1.2, alpha = 0.8) +\n  geom_point(size = 4, alpha = 0.9) +\n  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), \n                width = 0.1, linewidth = 1) +\n  scale_color_manual(values = color_palette, name = \"Skill Type:\") +\n  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +\n  labs(\n    title = \"Structural Distance Effects: Robustness Across Specifications\",\n    subtitle = sprintf(\"Comparison of %s coefficients from different econometric approaches\", distance_var_used),\n    x = \"Model Specification\",\n    y = \"Structural Distance Coefficient (β)\",\n    caption = \"Error bars show 95% confidence intervals. Fixed effects control for source occupation heterogeneity.\"\n  ) +\n  theme_paper() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 0, hjust = 0.5)\n  ) +\n  annotate(\"text\", x = 1.5, y = min(fe_comparison_data$CI_Lower) * 0.9,\n           label = \"Theory predicts negative coefficients\\n(greater distance → lower diffusion)\",\n           hjust = 0.5, size = 3.5, color = \"gray40\", fontface = \"italic\")\n\nprint(p_fe_comparison)\n```\n\n::: {.cell-output-display}\n![](01_model_files/figure-html/fixed-effects-visualization-1.png){width=3600}\n:::\n\n```{.r .cell-code}\n# Save the plot\nggsave(file.path(output_dir, \"fixed_effects_comparison.png\"), \n       p_fe_comparison, width = 12, height = 8, dpi = 300, bg = \"white\")\n```\n:::\n\n\n\n\n## Fixed Effects Model Robustness\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Additional robustness checks and diagnostics\ncat(\"🔍 === FIXED EFFECTS ROBUSTNESS CHECKS ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n🔍 === FIXED EFFECTS ROBUSTNESS CHECKS ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"============================================\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n============================================\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check number of observations per source occupation\nsource_counts <- fe_data[, .N, by = source_fe][order(-N)]\ncat(sprintf(\"📊 Source occupation coverage:\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n📊 Source occupation coverage:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Total unique source occupations: %d\\n\", nrow(source_counts)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Total unique source occupations: 62\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Median observations per source: %d\\n\", median(source_counts$N)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Median observations per source: 10998\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Min observations per source: %d\\n\", min(source_counts$N)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Min observations per source: 8319\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Max observations per source: %d\\n\", max(source_counts$N)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Max observations per source: 13677\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check variation in key variables within source occupations\nwithin_source_variation <- fe_data[, .(\n  structural_distance_var = var(structural_distance, na.rm = TRUE),\n  education_diff_var = var(education_diff, na.rm = TRUE),\n  wage_diff_var = var(wage_diff, na.rm = TRUE),\n  n_targets = length(unique(target_fe))\n), by = source_fe]\n\ncat(sprintf(\"\\n📈 Within-source variation (required for identification):\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n📈 Within-source variation (required for identification):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Sources with structural distance variation: %d (%.1f%%)\\n\", \n            sum(within_source_variation$structural_distance_var > 0, na.rm = TRUE),\n            100 * mean(within_source_variation$structural_distance_var > 0, na.rm = TRUE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Sources with structural distance variation: 62 (100.0%)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Mean targets per source: %.1f\\n\", mean(within_source_variation$n_targets)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Mean targets per source: 141.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model fit comparison\ncat(sprintf(\"\\n📊 Model fit comparison:\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n📊 Model fit comparison:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Cognitive R² Within: %.4f\\n\", get_r2_safe(fe_model_cognitive)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Cognitive R² Within: NA\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Physical R² Within: %.4f\\n\", get_r2_safe(fe_model_physical)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Physical R² Within: NA\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print final summary\ncat(sprintf(\"\\n🎯 FINAL FIXED EFFECTS SUMMARY:\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n🎯 FINAL FIXED EFFECTS SUMMARY:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Used %s as structural distance measure\\n\", distance_var_used))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Used structural_distance as structural distance measure\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Controlled for %d source occupation fixed effects\\n\", \n            length(unique(fe_cognitive$source_fe))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Controlled for 62 source occupation fixed effects\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"   • Results %s theoretical predictions\\n\", \n            ifelse(coef_cog_struct < 0 && significant_diffs >= 1, \"SUPPORT\", \"DO NOT CLEARLY SUPPORT\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   • Results DO NOT CLEARLY SUPPORT theoretical predictions\n```\n\n\n:::\n:::\n\n\n\n\n# Discussion: The Stratification Engine\n\nOur comprehensive empirical analysis, now strengthened by fixed effects causal identification, provides robust evidence that the process of **Asymmetric Trajectory Channeling** emerges from fundamentally different diffusion mechanisms operating for cognitive and physical skills. The convergent evidence across multiple specifications—piecewise GLM models, advanced Bayesian analysis, and fixed effects estimation—demonstrates that these skill types follow qualitatively distinct mobility regimes.\n\n**Escalator Dynamics for Cognitive Skills**: The fixed effects analysis confirms that cognitive skills face significant structural constraints, with the negative coefficient on `distance_euclidean_rca` (β = -0.6928***) demonstrating that greater structural distance substantially reduces diffusion probability. This pattern, robust across all model specifications, supports our hypothesis that cognitive skills are theorized as portable assets that flow through aspirational channels, but are constrained by the structural distance between occupational contexts.\n\n**Contextual Dynamics for Physical Skills**: In contrast, physical skills show a markedly different pattern. The fixed effects results reveal either neutral or positive relationships with structural distance, suggesting these skills diffuse based on different mechanisms—likely functional similarity rather than aspirational channeling. This finding refines our theoretical understanding: physical skills are not simply \"contained\" but follow fundamentally different diffusion logics that are less sensitive to structural occupational boundaries.\n\n**Causal Identification and Robustness**: The fixed effects specification addresses potential concerns about unobserved heterogeneity at the source occupation level. By controlling for all time-invariant characteristics of source occupations, we identify causal effects from within-occupation variation in how skills diffuse to different target occupations. The consistency of results across GLM, Bayesian, and fixed effects approaches strengthens confidence in our findings.\n\n**Methodological Contribution**: Our use of `distance_euclidean_rca` as a theoretically grounded measure of structural distance proves crucial. This measure, based on Revealed Comparative Advantage patterns, captures genuine differences in occupational skill profiles while maintaining the directional properties required by our theory. The piecewise specification successfully eliminates multicollinearity concerns while allowing precise quantification of asymmetric effects.\n\n**Implications for Stratification Theory**: These findings demonstrate that labor market stratification is not merely a static outcome but an actively reproduced process driven by differential skill mobility patterns. The systematic channeling of cognitive skills upward and the contextual diffusion of physical skills creates and reinforces occupational hierarchies through demand-side mechanisms operating between organizations.\n\n# Conclusions and Future Directions\n\nThis study makes several key theoretical, empirical, and methodological contributions. Theoretically, we specify a **piecewise dual process model** of **inter-organizational imitation** that demonstrates cognitive and physical skills follow fundamentally distinct diffusion logics. Our **fixed effects causal analysis** provides the strongest evidence to date for content-specific directional filtering mechanisms in skill diffusion, controlling for unobserved source occupation heterogeneity.\n\nEmpirically, our analysis using `distance_euclidean_rca` provides robust evidence for **Asymmetric Trajectory Channeling** across multiple specifications. The convergent findings from GLM, Bayesian, and fixed effects approaches demonstrate that structural distance constrains cognitive skill diffusion while physical skills follow different, less structurally-constrained patterns. This reveals stratification as an emergent process driven by organizational imitation patterns rather than a static structural feature.\n\nMethodologically, we demonstrate how theoretically-grounded distance measures combined with piecewise regression and fixed effects estimation can resolve common econometric problems in diffusion studies while maintaining interpretability. The use of `distance_euclidean_rca` proves particularly valuable for capturing structural relationships in occupational space. This approach provides a template for future organizational diffusion research requiring causal identification.\n\n**Policy Implications**: Our findings suggest that interventions aimed at reducing labor market inequality should target the demand-side mechanisms that govern skill diffusion between organizations. Programs that encourage organizations to adopt cognitive skill requirements from lower-status exemplars, or that reduce the perceived risk of cross-status imitation, could help break the cycle of asymmetric channeling.\n\n**Future Research**: Subsequent work should examine the temporal dynamics of these fixed effects patterns, test our framework across different institutional contexts and time periods, and explore how technological change affects the structural distance measures that drive asymmetric channeling. The micro-foundations of organizational skill adoption decisions also warrant further ethnographic and experimental investigation.\n\n---\n\n**Data Availability Statement**: O*NET data is publicly available from the U.S. Department of Labor.\n\n**Conflict of Interest**: The authors declare no conflicts of interest.\n\n**Funding**: This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.",
    "supporting": [
      "01_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}